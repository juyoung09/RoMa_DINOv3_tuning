#!/usr/bin/env python3
"""
DINOv2 vs DINOv3 RoMa ëª¨ë¸ ë¹„êµ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ (100ê°œ ëœë¤ ìƒ˜í”Œ)
"""

import os
import sys
import json
import cv2
import numpy as np
import torch
import random
from PIL import Image
from tqdm import tqdm
try:
    import pandas as pd
except ImportError:
    print("pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install pandasë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    exit(1)

# ============================================================================
# ì„¤ì • (Configuration)
# ============================================================================
class Config:
    # --- ë°ì´í„° ë° ëª¨ë¸ ê²½ë¡œ ---
    DATA_DIR = "./processed_refine_data"  # ì „ì²˜ë¦¬ëœ ë°ì´í„° í´ë”
    LABEL_MAPPING_FILE = os.path.join(DATA_DIR, "label_mapping.json")
    
    # ë¹„êµí•  ëª¨ë¸ë“¤ (ë³„ë„ í´ë”ì—ì„œ ë¡œë“œ)
    ORIGINAL_ROMA_DIR = "./RoMa_original"
    DINOV3_ROMA_DIR = "./RoMa_dinov3"
    
    # --- í‰ê°€ ì„¤ì • ---
    TARGET_SIZE = 560  # ì´ë¯¸ì§€ë¥¼ ë¦¬ì‚¬ì´ì¦ˆí•  í¬ê¸°
    RANSAC_THRESHOLD = 3.0  # RANSAC ì´ìƒì¹˜ ì œê±° ì„ê³„ê°’
    MIN_MATCHES = 8  # Homography ì¶”ì •ì„ ìœ„í•œ ìµœì†Œ ë§¤ì¹­ ìˆ˜
    NUM_RANDOM_SAMPLES = 100  # ëœë¤ ìƒ˜í”Œ ìˆ˜

    # --- í‰ê°€ì§€í‘œ ì„ê³„ê°’ ---
    PCK_THRESHOLDS = [1, 3, 5]  # PCK ê³„ì‚°ì„ ìœ„í•œ í”½ì…€ ì„ê³„ê°’
    SUCCESS_THRESHOLDS = [5, 10] # Success Rate ê³„ì‚°ì„ ìœ„í•œ RMSE ì„ê³„ê°’


# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (Utility Functions)
# ============================================================================
def load_json_file(filepath):
    """JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found: {filepath}")
    with open(filepath, 'r') as f:
        return json.load(f)

def get_random_samples(data_dir, num_samples=100):
    """ë°ì´í„°ì…‹ì—ì„œ ëœë¤ ìƒ˜í”Œì„ ì„ ì •í•©ë‹ˆë‹¤."""
    # ëª¨ë“  pair ë””ë ‰í† ë¦¬ ì°¾ê¸°
    pair_dirs = [d for d in os.listdir(data_dir) if d.startswith('pair_')]
    pair_dirs.sort()
    
    print(f"ì´ {len(pair_dirs)}ê°œì˜ pair ì¤‘ì—ì„œ {num_samples}ê°œë¥¼ ëœë¤ìœ¼ë¡œ ì„ ì •í•©ë‹ˆë‹¤.")
    
    # ëœë¤ ìƒ˜í”Œë§
    random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ì‹œë“œ ì„¤ì •
    selected_pairs = random.sample(pair_dirs, min(num_samples, len(pair_dirs)))
    
    # pair ë²ˆí˜¸ ì¶”ì¶œ
    selected_indices = []
    for pair_dir in selected_pairs:
        pair_num = int(pair_dir.split('_')[1])
        selected_indices.append(pair_num)
    
    selected_indices.sort()
    
    print(f"ì„ ì •ëœ ìƒ˜í”Œ: {selected_indices[:10]}... (ì´ {len(selected_indices)}ê°œ)")
    
    return selected_indices

def parse_label_file(filepath):
    """ë¼ë²¨ íŒŒì¼ì—ì„œ ëŒ€ì‘ì  ì¢Œí‘œë¥¼ ì½ì–´ì˜µë‹ˆë‹¤."""
    try:
        points = []
        with open(filepath, 'r') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line and not line.startswith('#'):
                    parts = line.split()
                    if len(parts) >= 2:
                        try:
                            # í˜•ì‹: "1 296.0030653586301, 188.0888049593124"
                            if len(parts) >= 3:
                                x_str = parts[1].rstrip(',')  # ì½¤ë§ˆ ì œê±°
                                y_str = parts[2]
                                x, y = float(x_str), float(y_str)
                                points.append([x, y])
                        except (ValueError, IndexError) as e:
                            print(f"Warning: Skipping line {line_num} in {filepath}: {line} (error: {e})")
                            continue
        return np.array(points) if points else None
    except Exception as e:
        print(f"Error parsing {filepath}: {e}")
        return None

def get_gt_homography_from_labels(pair_idx, label_mapping, target_size, base_dir="."):
    """ë¼ë²¨ íŒŒì¼ì—ì„œ GT í˜¸ëª¨ê·¸ë˜í”¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        pair_key = str(pair_idx)
        if pair_key not in label_mapping:
            return None, None, None
        
        labels = label_mapping[pair_key]
        
        # ë¼ë²¨ íŒŒì¼ ê²½ë¡œë¥¼ í˜„ì¬ í´ë”ì˜ labels ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •
        kompsat_filename = os.path.basename(labels['kompsat_label'])
        google_filename = os.path.basename(labels['google_label'])
        
        # base_dirì„ ì‚¬ìš©í•˜ì—¬ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
        kompsat_label_path = os.path.join(base_dir, "labels", kompsat_filename)
        google_label_path = os.path.join(base_dir, "labels", google_filename)
        
        # ë¼ë²¨ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not (os.path.exists(kompsat_label_path) and os.path.exists(google_label_path)):
            return None, None, None
        
        # ëŒ€ì‘ì  ë¡œë“œ
        k_points = parse_label_file(kompsat_label_path)
        g_points = parse_label_file(google_label_path)
        
        if k_points is None or g_points is None:
            return None, None, None
        
        # ì  ê°œìˆ˜ê°€ ë‹¤ë¥¸ ê²½ìš° ë” ì ì€ ê°œìˆ˜ì— ë§ì¶¤
        min_points = min(len(k_points), len(g_points))
        if min_points < 4:
            return None, None, None
        
        # ë” ì ì€ ê°œìˆ˜ë§Œí¼ë§Œ ì‚¬ìš©
        k_points = k_points[:min_points]
        g_points = g_points[:min_points]
        
        # RANSACì„ ì‚¬ìš©í•´ì„œ ì´ìƒì¹˜ ì œê±° ë° í˜¸ëª¨ê·¸ë˜í”¼ ì¶”ì •
        best_H = None
        best_mask = None
        best_inliers = 0
        
        for threshold in [5.0, 8.0, 12.0]:  # ë” ê´€ëŒ€í•œ ì„ê³„ê°’ ì‹œë„
            H, mask = cv2.findHomography(g_points, k_points, cv2.RANSAC, threshold)
            if H is not None and mask is not None:
                inliers = np.sum(mask)
                if inliers > best_inliers and inliers >= 6:  # ìµœì†Œ 6ê°œ ì´ìƒì˜ inlier í•„ìš”
                    best_H = H
                    best_mask = mask
                    best_inliers = inliers
        
        if best_H is None or best_inliers < 6:
            return None, None, None
        
        # RANSACìœ¼ë¡œ ì„ íƒëœ inlierë“¤ë§Œ ì‚¬ìš©
        inlier_indices = np.where(best_mask.ravel())[0]
        k_points_filtered = k_points[inlier_indices]
        g_points_filtered = g_points[inlier_indices]
        
        # í•„í„°ë§ëœ ì ë“¤ë¡œ ìµœì¢… í˜¸ëª¨ê·¸ë˜í”¼ ì¬ê³„ì‚°
        H_gt, _ = cv2.findHomography(g_points_filtered, k_points_filtered, 0)
        
        if H_gt is None:
            return None, None, None
        
        # GT ì¢Œí‘œë¥¼ ëª¨ë¸ ì…ë ¥ í¬ê¸°(560x560)ì— ë§ê²Œ ìŠ¤ì¼€ì¼ë§
        original_size = 1000  # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ì¶”ì •
        scale_factor = target_size / original_size
        
        g_points_scaled = g_points_filtered * scale_factor
        k_points_scaled = k_points_filtered * scale_factor
        
        # í˜¸ëª¨ê·¸ë˜í”¼ë„ ìŠ¤ì¼€ì¼ë§ì— ë§ê²Œ ì¡°ì •
        H_gt_scaled = H_gt.copy()
        H_gt_scaled[0, 2] *= scale_factor  # tx
        H_gt_scaled[1, 2] *= scale_factor  # ty
            
        return H_gt_scaled, g_points_scaled, k_points_scaled
        
    except Exception as e:
        return None, None, None

def decompose_homography(H):
    """Homography í–‰ë ¬ì„ ì´ë™, íšŒì „, í¬ê¸° ì„±ë¶„ìœ¼ë¡œ ë¶„í•´í•©ë‹ˆë‹¤."""
    if H is None:
        return np.nan, np.nan, np.nan

    tx, ty = H[0, 2], H[1, 2]
    translation = np.sqrt(tx**2 + ty**2)
    
    a, b, c, d = H[0, 0], H[0, 1], H[1, 0], H[1, 1]
    
    # íšŒì „ê° (atan2 ì‚¬ìš©)
    rotation = np.arctan2(c, a) * (180 / np.pi)
    
    # í¬ê¸° (Scale)
    scale = np.sqrt(a**2 + c**2)
    
    return translation, rotation, scale

def calculate_metrics(H_pred, gt_points_source, gt_points_target):
    """
    í•˜ë‚˜ì˜ ì´ë¯¸ì§€ ìŒì— ëŒ€í•œ ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    H_pred: ëª¨ë¸ì´ ì˜ˆì¸¡í•œ Homography
    gt_points_source: ë³€í™˜ì˜ ê¸°ì¤€ì´ ë˜ëŠ” GT ì¢Œí‘œ (ì˜ˆ: Google ì´ë¯¸ì§€ GT ì¢Œí‘œ)
    gt_points_target: ë³€í™˜ì˜ ëª©í‘œê°€ ë˜ëŠ” GT ì¢Œí‘œ (ì˜ˆ: Kompsat ì´ë¯¸ì§€ GT ì¢Œí‘œ)
    """
    if H_pred is None or gt_points_source is None:
        return None

    # 1. GT ì¢Œí‘œë¥¼ H_predë¡œ ë³€í™˜í•˜ì—¬ ì˜ˆì¸¡ ì¢Œí‘œ ìƒì„±
    gt_source_h = np.hstack([gt_points_source, np.ones((len(gt_points_source), 1))])
    pred_target_h = (H_pred @ gt_source_h.T).T
    pred_target_points = pred_target_h[:, :2] / pred_target_h[:, 2:3]
    
    # 2. í”½ì…€ ì˜¤ì°¨(error) ê³„ì‚°
    errors = np.linalg.norm(pred_target_points - gt_points_target, axis=1)
    
    # 3. í‰ê°€ì§€í‘œ ê³„ì‚°
    rmse = np.sqrt(np.mean(errors ** 2))
    
    pck_metrics = {}
    for t in Config.PCK_THRESHOLDS:
        pck_metrics[f'pck@{t}px'] = (errors < t).mean() * 100
        
    trans_err, rot_err, scale_err = decompose_homography(H_pred)
    
    metrics = {
        'rmse': rmse,
        'mean_error': np.mean(errors),
        'median_error': np.median(errors),
        'std_error': np.std(errors),
        **pck_metrics,
        'translation_error': trans_err,
        'rotation_error': rot_err,
        'scale_error': abs(scale_err - 1.0) # ìŠ¤ì¼€ì¼ì€ 1ì´ ê¸°ì¤€ì´ë¯€ë¡œ 1ê³¼ì˜ ì°¨ì´
    }
    return metrics

def evaluate_model_in_directory(model_dir, model_name, test_indices, label_mapping, device):
    """íŠ¹ì • ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  í‰ê°€"""
    
    print(f"\n{'='*80}")
    print(f"ğŸ”„ {model_name} í‰ê°€ ì‹œì‘")
    print('='*80)
    
    # í•´ë‹¹ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
    original_cwd = os.getcwd()
    os.chdir(model_dir)
    
    try:
        # í•´ë‹¹ ë””ë ‰í† ë¦¬ì˜ ëª¨ë¸ import
        sys.path.insert(0, '.')
        
        if "dinov3" in model_name.lower():
            # DINOv3 ë””ë ‰í† ë¦¬ë¡œ ì´ë™
            dinov3_dir = os.path.join(original_cwd, 'RoMa_dinov3')
            os.chdir(dinov3_dir)
            sys.path.insert(0, '.')
            from romatch.models.model_zoo.roma_models import roma_model
            # DINOv3 ê¸°ë°˜ RoMa ëª¨ë¸ ìƒì„±
            model = roma_model(
                resolution=(560, 560),
                upsample_preds=True,
                device=device,
                weights=None,  # ê°€ì¤‘ì¹˜ ì—†ì´ í…ŒìŠ¤íŠ¸
                dinov3_model_name="facebook/dinov3-vit7b16-pretrain-sat493m",
                amp_dtype=torch.float32
            )
            os.chdir(model_dir)  # ë‹¤ì‹œ ì›ë˜ ë””ë ‰í† ë¦¬ë¡œ
        else:
            from romatch import roma_outdoor
            # ì›ë³¸ RoMa ëª¨ë¸ ë¡œë“œ
            model = roma_outdoor(device=device)
            # ê°€ì¤‘ì¹˜ ë¡œë“œ (ì›ë³¸ RoMaì˜ ê²½ìš°)
            model.load_state_dict(torch.load('./checkpoints/roma_outdoor.pth', map_location=device))
        
        model.eval()
        print(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        all_results = []
        
        for pair_idx in tqdm(test_indices, desc=f"{model_name} í‰ê°€"):
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ
                pair_dir = os.path.join('../processed_refine_data', f'pair_{pair_idx:04d}')
                img_source = cv2.cvtColor(cv2.imread(os.path.join(pair_dir, 'google.jpg')), cv2.COLOR_BGR2RGB)
                img_target = cv2.cvtColor(cv2.imread(os.path.join(pair_dir, 'kompsat.jpg')), cv2.COLOR_BGR2RGB)
                
                h, w = img_source.shape[:2]
                
                # GT Homography ë° GT ì¢Œí‘œ ë¡œë“œ (ë¼ë²¨ íŒŒì¼ ì‚¬ìš©)
                _, gt_source_pts, gt_target_pts = get_gt_homography_from_labels(pair_idx, label_mapping, Config.TARGET_SIZE, base_dir=original_cwd)
                if gt_source_pts is None:
                    continue

                # ëª¨ë¸ ì¶”ë¡  (RoMa ê¸°ì¤€)
                with torch.no_grad():
                    warp, certainty = model.match(Image.fromarray(img_source), Image.fromarray(img_target), device=device)
                    matches, _ = model.sample(warp, certainty)
                
                if matches is None or len(matches) == 0:
                    continue
                    
                kps_source, kps_target = model.to_pixel_coordinates(matches, h, w, h, w)
                kps_source = kps_source.cpu().numpy()
                kps_target = kps_target.cpu().numpy()

                if len(kps_source) < Config.MIN_MATCHES:
                    continue
                
                # ì˜ˆì¸¡ Homography(H_pred) ì¶”ì •
                H_pred, _ = cv2.findHomography(kps_source, kps_target, cv2.RANSAC, Config.RANSAC_THRESHOLD)
                
                # í‰ê°€ì§€í‘œ ê³„ì‚°
                metrics = calculate_metrics(H_pred, gt_source_pts, gt_target_pts)
                if metrics:
                    metrics['model_name'] = model_name
                    all_results.append(metrics)

            except Exception as e:
                if pair_idx < 10:  # ì²˜ìŒ 10ê°œ ì—ëŸ¬ ì¶œë ¥
                    print(f"  ìƒ˜í”Œ {pair_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
                
        print(f"âœ… {model_name} í‰ê°€ ì™„ë£Œ: {len(all_results)}ê°œ ìƒ˜í”Œ")
        return all_results
        
    finally:
        os.chdir(original_cwd)

# ============================================================================
# ë©”ì¸ í‰ê°€ í•¨ìˆ˜ (Main Evaluation Function)
# ============================================================================
def compare_models():
    # --- 1. ì´ˆê¸° ì„¤ì • ---
    print("=" * 80)
    print("ğŸš€ ì›ë³¸ RoMa (DINOv2) vs DINOv3 RoMa ë¹„êµ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    print(f"ğŸ“Š ëœë¤ ìƒ˜í”Œ ìˆ˜: {Config.NUM_RANDOM_SAMPLES}ê°œ")
    print("=" * 80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ì‚¬ìš© ì¥ì¹˜: {device}")
    
    # --- 2. ëœë¤ ìƒ˜í”Œ ì„ ì • ---
    print("\n[1/4] ëœë¤ ìƒ˜í”Œ ì„ ì • ì¤‘...")
    test_indices = get_random_samples(Config.DATA_DIR, Config.NUM_RANDOM_SAMPLES)
    
    # ë¼ë²¨ ë§¤í•‘ ë¡œë“œ
    label_mapping = load_json_file(Config.LABEL_MAPPING_FILE)
    
    print(f"âœ… {len(test_indices)}ê°œì˜ ëœë¤ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì„ ì • ì™„ë£Œ.")
    print(f"âœ… {len(label_mapping)}ê°œì˜ ë¼ë²¨ ë§¤í•‘ ë¡œë“œ ì™„ë£Œ.")
    
    # --- 3. ëª¨ë¸ë³„ í‰ê°€ ---
    print("\n[2/4] ëª¨ë¸ë³„ í‰ê°€ ì§„í–‰ ì¤‘...")
    
    original_results = evaluate_model_in_directory(
        Config.ORIGINAL_ROMA_DIR, 
        "ì›ë³¸ RoMa (DINOv2)", 
        test_indices, 
        label_mapping,
        device
    )
    
    dinov3_results = evaluate_model_in_directory(
        Config.DINOV3_ROMA_DIR, 
        "DINOv3 RoMa", 
        test_indices, 
        label_mapping,
        device
    )
    
    # --- 4. ê²°ê³¼ ë¹„êµ ë° ì¶œë ¥ ---
    print("\n[3/4] ê²°ê³¼ ë¹„êµ ë° ë¶„ì„ ì¤‘...")
    print_comparison_results(original_results, dinov3_results)
    
    print("\n[4/4] ë¹„êµ í‰ê°€ ì™„ë£Œ!")

def print_comparison_results(original_results, dinov3_results):
    """ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
    
    print(f"\n{'='*80}")
    print("ğŸ“Š ì›ë³¸ RoMa (DINOv2) vs DINOv3 RoMa ë¹„êµ ê²°ê³¼")
    print('='*80)
    
    print(f"\nValid samples:")
    print(f"  ì›ë³¸ RoMa (DINOv2): {len(original_results)}")
    print(f"  DINOv3 RoMa: {len(dinov3_results)}")
    
    if len(original_results) == 0 and len(dinov3_results) == 0:
        print("\nâš ï¸  No valid results!")
        return
    
    # ê° ëª¨ë¸ë³„ í†µê³„ ê³„ì‚°
    models_data = []
    
    for name, results in [
        ("ì›ë³¸ RoMa (DINOv2)", original_results),
        ("DINOv3 RoMa", dinov3_results)
    ]:
        if len(results) == 0:
            models_data.append({
                'name': name,
                'count': 0,
                'rmse_mean': None,
                'rmse_std': None,
                'rmse_median': None,
                'pck_1px': None,
                'pck_3px': None,
                'pck_5px': None,
                'translation_error': None,
                'rotation_error': None,
                'scale_error': None,
            })
            continue
        
        # í†µê³„ ê³„ì‚°
        rmses = [r['rmse'] for r in results]
        pck_1px = [r['pck@1px'] for r in results]
        pck_3px = [r['pck@3px'] for r in results]
        pck_5px = [r['pck@5px'] for r in results]
        translation_errors = [r['translation_error'] for r in results]
        rotation_errors = [r['rotation_error'] for r in results]
        scale_errors = [r['scale_error'] for r in results]
        
        models_data.append({
            'name': name,
            'count': len(results),
            'rmse_mean': np.mean(rmses),
            'rmse_std': np.std(rmses),
            'rmse_median': np.median(rmses),
            'pck_1px': np.mean(pck_1px),
            'pck_3px': np.mean(pck_3px),
            'pck_5px': np.mean(pck_5px),
            'translation_error': np.mean(translation_errors),
            'rotation_error': np.mean(rotation_errors),
            'scale_error': np.mean(scale_errors),
        })
    
    # í‘œ í˜•ì‹ ì¶œë ¥
    print(f"\n{'='*80}")
    print("ğŸ“Š ì„±ëŠ¥ ë¹„êµí‘œ (Performance Comparison Table)")
    print('='*80)
    
    # í—¤ë”
    print(f"\n{'Metric':<40} | {'ì›ë³¸ RoMa (DINOv2)':>20} | {'DINOv3 RoMa':>15}")
    print('-' * 85)
    
    # ìƒ˜í”Œ ìˆ˜
    print(f"{'Valid Samples':<40} | {models_data[0]['count']:>20} | {models_data[1]['count']:>15}")
    print('-' * 85)
    
    # RMSE
    if models_data[0]['rmse_mean'] is not None and models_data[1]['rmse_mean'] is not None:
        print(f"{'RMSE Mean (px)':<40} | {models_data[0]['rmse_mean']:>19.2f} | {models_data[1]['rmse_mean']:>14.2f}")
        print(f"{'RMSE Std Dev (px)':<40} | {models_data[0]['rmse_std']:>19.2f} | {models_data[1]['rmse_std']:>14.2f}")
        print(f"{'RMSE Median (px)':<40} | {models_data[0]['rmse_median']:>19.2f} | {models_data[1]['rmse_median']:>14.2f}")
    else:
        print(f"{'RMSE Mean (px)':<40} | {'N/A':>20} | {'N/A':>15}")
    print('-' * 85)
    
    # PCK
    if models_data[0]['pck_1px'] is not None and models_data[1]['pck_1px'] is not None:
        print(f"{'PCK@1px (%)':<40} | {models_data[0]['pck_1px']:>19.2f} | {models_data[1]['pck_1px']:>14.2f}")
        print(f"{'PCK@3px (%)':<40} | {models_data[0]['pck_3px']:>19.2f} | {models_data[1]['pck_3px']:>14.2f}")
        print(f"{'PCK@5px (%)':<40} | {models_data[0]['pck_5px']:>19.2f} | {models_data[1]['pck_5px']:>14.2f}")
    else:
        print(f"{'PCK@1px (%)':<40} | {'N/A':>20} | {'N/A':>15}")
    print('-' * 85)
    
    # ê¸°í•˜í•™ì  ì˜¤ì°¨
    if models_data[0]['translation_error'] is not None and models_data[1]['translation_error'] is not None:
        print(f"{'Translation Error (px)':<40} | {models_data[0]['translation_error']:>19.2f} | {models_data[1]['translation_error']:>14.2f}")
        print(f"{'Rotation Error (deg)':<40} | {models_data[0]['rotation_error']:>19.2f} | {models_data[1]['rotation_error']:>14.2f}")
        print(f"{'Scale Error':<40} | {models_data[0]['scale_error']:>19.2f} | {models_data[1]['scale_error']:>14.2f}")
    else:
        print(f"{'Translation Error (px)':<40} | {'N/A':>20} | {'N/A':>15}")
    
    print('='*85)
    
    # ì„±ëŠ¥ ê°œì„  ìš”ì•½
    if models_data[1]['rmse_mean'] is not None and models_data[0]['rmse_mean'] is not None:
        print(f"\n{'='*80}")
        print("ğŸ“ˆ ì„±ëŠ¥ ê°œì„  ìš”ì•½ (Performance Improvement Summary)")
        print('='*80)
        
        rmse_improvement = ((models_data[0]['rmse_mean'] - models_data[1]['rmse_mean']) / models_data[0]['rmse_mean']) * 100
        pck_1px_improvement = models_data[1]['pck_1px'] - models_data[0]['pck_1px']
        pck_3px_improvement = models_data[1]['pck_3px'] - models_data[0]['pck_3px']
        pck_5px_improvement = models_data[1]['pck_5px'] - models_data[0]['pck_5px']
        
        print(f"\nì›ë³¸ RoMa (DINOv2) â†’ DINOv3 RoMa:")
        print(f"  RMSE: {models_data[0]['rmse_mean']:.2f}px â†’ {models_data[1]['rmse_mean']:.2f}px ({rmse_improvement:+.2f}%)")
        print(f"  PCK@1px: {models_data[0]['pck_1px']:.2f}% â†’ {models_data[1]['pck_1px']:.2f}% ({pck_1px_improvement:+.2f}%p)")
        print(f"  PCK@3px: {models_data[0]['pck_3px']:.2f}% â†’ {models_data[1]['pck_3px']:.2f}% ({pck_3px_improvement:+.2f}%p)")
        print(f"  PCK@5px: {models_data[0]['pck_5px']:.2f}% â†’ {models_data[1]['pck_5px']:.2f}% ({pck_5px_improvement:+.2f}%p)")
        
        print('='*80)


if __name__ == "__main__":
    # --- ì‚¬ìš© ì˜ˆì‹œ ---
    # í„°ë¯¸ë„ì—ì„œ `python compare_roma_models_100.py`ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    compare_models()
