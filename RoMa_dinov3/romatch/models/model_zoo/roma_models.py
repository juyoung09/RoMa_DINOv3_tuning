import warnings
import torch.nn as nn
import torch
from romatch.models.matcher import *
from romatch.models.transformer import Block, TransformerDecoder, MemEffAttention
from romatch.models.encoders import *
from romatch.models.encoders import CNNandDinov3
from romatch.models.tiny import TinyRoMa

def tiny_roma_v1_model(weights = None, freeze_xfeat=False, exact_softmax=False, xfeat = None):
    model = TinyRoMa(
        xfeat = xfeat,
        freeze_xfeat=freeze_xfeat, 
        exact_softmax=exact_softmax)
    if weights is not None:
        model.load_state_dict(weights)
    return model

def load_roma_weights_except_coarse(model, original_weights_path):
    """
    ÏõêÎ≥∏ RoMa Í∞ÄÏ§ëÏπòÎ•º Î°úÎìúÌïòÎêò, coarse level (DINOv3 Î∂ÄÎ∂Ñ)ÏùÄ Ï†úÏô∏ÌïòÍ≥† ÎÇòÎ®∏ÏßÄ Î∂ÄÎ∂ÑÎßå Î°úÎìúÌï©ÎãàÎã§.
    """
    print("üîÑ ÏõêÎ≥∏ RoMa Í∞ÄÏ§ëÏπò Î°úÎìú Ï§ë (DINOv3 coarse level Ï†úÏô∏)...")
    
    # ÏõêÎ≥∏ Í∞ÄÏ§ëÏπò Î°úÎìú
    original_weights = torch.load(original_weights_path, map_location='cpu')
    
    # ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞
    current_state_dict = model.state_dict()
    
    # Î°úÎìúÌï† Í∞ÄÏ§ëÏπò ÌïÑÌÑ∞ÎßÅ (DINOv3 Í¥ÄÎ†® Î∂ÄÎ∂Ñ Ï†úÏô∏)
    filtered_weights = {}
    skipped_keys = []
    
    for key, value in original_weights.items():
        # DINOv3 Í¥ÄÎ†® ÌÇ§Îì§ Ï†úÏô∏ (Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ selective Î°úÎî©)
        if any(skip_pattern in key for skip_pattern in [
            'encoder.cnn.',  # VGG19 Î∂ÄÎ∂ÑÏùÄ Ï†úÏô∏ (DINOv3Î°ú ÎåÄÏ≤¥Îê®)
            'encoder.dinov2_model',  # DINOv2 Î™®Îç∏ Î∂ÄÎ∂Ñ (Í∏∞Ï°¥)
            'encoder.dinov3_model',  # DINOv3 Î™®Îç∏ Î∂ÄÎ∂Ñ (ÏÉàÎ°ú Ï∂îÍ∞Ä)
            'encoder._dinov3_model',  # DINOv3 Î™®Îç∏ Î∂ÄÎ∂Ñ (ÏÉàÎ°ú Ï∂îÍ∞Ä)
            # 'decoder.proj.16.0.weight',  # proj16ÏùÄ ÎèôÏ†Å ÏÑ§Ï†ïÏúºÎ°ú Î°úÎìú Í∞ÄÎä•
        ]):
            skipped_keys.append(key)
            continue
            
        # ÌòÑÏû¨ Î™®Îç∏Ïóê Ìï¥Îãπ ÌÇ§Í∞Ä ÏûàÍ≥† ÌÅ¨Í∏∞Í∞Ä ÎßûÎäî Í≤ΩÏö∞Îßå Î°úÎìú
        if key in current_state_dict:
            if current_state_dict[key].shape == value.shape:
                filtered_weights[key] = value
            else:
                # proj16 ÌÇ§Îäî Í∞ïÏ†ú Î°úÎìú/Î≥ÄÌôò (ÎèôÏ†Å Ï∞®ÏõêÏúºÎ°ú Ïù∏Ìïú ÌÅ¨Í∏∞ Î∂àÏùºÏπò ÌóàÏö©)
                if 'proj.16' in key and value.shape[1] != current_state_dict[key].shape[1]:
                    print(f"  ‚ö†Ô∏è  proj16 ÌÅ¨Í∏∞ Î∂àÏùºÏπòÎ°ú Í±¥ÎÑàÎúÄ: {key} (ÏõêÎ≥∏: {value.shape}, ÌòÑÏû¨: {current_state_dict[key].shape})")
                    skipped_keys.append(key)
                    continue
                else:
                    print(f"  ‚ö†Ô∏è  ÌÅ¨Í∏∞ Î∂àÏùºÏπòÎ°ú Í±¥ÎÑàÎúÄ: {key} (ÏõêÎ≥∏: {value.shape}, ÌòÑÏû¨: {current_state_dict[key].shape})")
                    skipped_keys.append(key)
        else:
            print(f"  ‚ö†Ô∏è  ÌÇ§Í∞Ä ÏóÜÏñ¥ÏÑú Í±¥ÎÑàÎúÄ: {key}")
            skipped_keys.append(key)
    
    # ÌïÑÌÑ∞ÎßÅÎêú Í∞ÄÏ§ëÏπò Î°úÎìú
    model.load_state_dict(filtered_weights, strict=False)
    
    print(f"‚úÖ Í∞ÄÏ§ëÏπò Î°úÎìú ÏôÑÎ£å!")
    print(f"  - Î°úÎìúÎêú ÌÇ§: {len(filtered_weights)}Í∞ú")
    print(f"  - Í±¥ÎÑàÎõ¥ ÌÇ§: {len(skipped_keys)}Í∞ú")
    
    if skipped_keys:
        print(f"  - Í±¥ÎÑàÎõ¥ ÌÇ§Îì§: {skipped_keys[:5]}{'...' if len(skipped_keys) > 5 else ''}")
    
    return model, skipped_keys

def roma_model(resolution, upsample_preds, device = None, weights=None, dinov3_model_name="facebook/dinov3-vit7b16-pretrain-sat493m", amp_dtype: torch.dtype=torch.float16, original_roma_weights=None, **kwargs):
    # ÏûÖÎ†• Ìï¥ÏÉÅÎèÑ 512x512Î°ú Î≥ÄÍ≤Ω (Ìå®Ïπò 16 Î∞∞Ïàò)
    resolution = (512, 512)
    h, w = resolution
    
    # romatch weights and dinov3 weights are loaded seperately, as dinov3 weights are not parameters
    #torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul TODO: these probably ruin stuff, should be careful
    #torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn
    warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')
    gp_dim = 512
    feat_dim = 512
    decoder_dim = gp_dim + feat_dim
    cls_to_coord_res = 64
    coordinate_decoder = TransformerDecoder(
        nn.Sequential(*[Block(decoder_dim, 8, attn_class=MemEffAttention) for _ in range(5)]), 
        decoder_dim, 
        cls_to_coord_res**2 + 1,
        is_classifier=True,
        amp = True,
        pos_enc = False,)
    dw = True
    hidden_blocks = 8
    kernel_size = 5
    displacement_emb = "linear"
    disable_local_corr_grad = True
    
    conv_refiner = nn.ModuleDict(
        {
            "16": ConvRefiner(
                2 * 512+128+(2*7+1)**2,
                2 * 512+128+(2*7+1)**2,
                2 + 1,
                kernel_size=kernel_size,
                dw=dw,
                hidden_blocks=hidden_blocks,
                displacement_emb=displacement_emb,
                displacement_emb_dim=128,
                local_corr_radius = 7,
                corr_in_other = True,
                amp = True,
                disable_local_corr_grad = disable_local_corr_grad,
                bn_momentum = 0.01,
            ),
            "8": ConvRefiner(
                2 * 512+64+(2*3+1)**2,
                2 * 512+64+(2*3+1)**2,
                2 + 1,
                kernel_size=kernel_size,
                dw=dw,
                hidden_blocks=hidden_blocks,
                displacement_emb=displacement_emb,
                displacement_emb_dim=64,
                local_corr_radius = 3,
                corr_in_other = True,
                amp = True,
                disable_local_corr_grad = disable_local_corr_grad,
                bn_momentum = 0.01,
            ),
            "4": ConvRefiner(
                2 * 256+32+(2*2+1)**2,
                2 * 256+32+(2*2+1)**2,
                2 + 1,
                kernel_size=kernel_size,
                dw=dw,
                hidden_blocks=hidden_blocks,
                displacement_emb=displacement_emb,
                displacement_emb_dim=32,
                local_corr_radius = 2,
                corr_in_other = True,
                amp = True,
                disable_local_corr_grad = disable_local_corr_grad,
                bn_momentum = 0.01,
            ),
            "2": ConvRefiner(
                2 * 64+16,
                128+16,
                2 + 1,
                kernel_size=kernel_size,
                dw=dw,
                hidden_blocks=hidden_blocks,
                displacement_emb=displacement_emb,
                displacement_emb_dim=16,
                amp = True,
                disable_local_corr_grad = disable_local_corr_grad,
                bn_momentum = 0.01,
            ),
            "1": ConvRefiner(
                2 * 9 + 6,
                24,
                2 + 1,
                kernel_size=kernel_size,
                dw=dw,
                hidden_blocks = hidden_blocks,
                displacement_emb = displacement_emb,
                displacement_emb_dim = 6,
                amp = True,
                disable_local_corr_grad = disable_local_corr_grad,
                bn_momentum = 0.01,
            ),
        }
    )
    kernel_temperature = 0.2
    learn_temperature = False
    no_cov = True
    kernel = CosKernel
    only_attention = False
    basis = "fourier"
    gp16 = GP(
        kernel,
        T=kernel_temperature,
        learn_temperature=learn_temperature,
        only_attention=only_attention,
        gp_dim=gp_dim,
        basis=basis,
        no_cov=no_cov,
    )
    gps = nn.ModuleDict({"16": gp16})
    
    # ÎèôÏ†Å proj16 (DINOv3 Ï∞®Ïõê 4096 Ï†ÅÏùë)
    from transformers import AutoModel
    dinov3_model = AutoModel.from_pretrained(dinov3_model_name)
    hidden_size = dinov3_model.config.hidden_size  # 4096
    proj16 = nn.Sequential(nn.Conv2d(hidden_size, 512, 1, 1), nn.BatchNorm2d(512))
    proj8 = nn.Sequential(nn.Conv2d(512, 512, 1, 1), nn.BatchNorm2d(512))
    proj4 = nn.Sequential(nn.Conv2d(256, 256, 1, 1), nn.BatchNorm2d(256))
    proj2 = nn.Sequential(nn.Conv2d(128, 64, 1, 1), nn.BatchNorm2d(64))
    proj1 = nn.Sequential(nn.Conv2d(64, 9, 1, 1), nn.BatchNorm2d(9))
    proj = nn.ModuleDict({
        "16": proj16,
        "8": proj8,
        "4": proj4,
        "2": proj2,
        "1": proj1,
        })
    displacement_dropout_p = 0.0
    gm_warp_dropout_p = 0.0
    decoder = Decoder(coordinate_decoder, 
                      gps, 
                      proj, 
                      conv_refiner, 
                      detach=True, 
                      scales=["16", "8", "4", "2", "1"], 
                      displacement_dropout_p = displacement_dropout_p,
                      gm_warp_dropout_p = gm_warp_dropout_p)
    
    encoder = CNNandDinov3(
        cnn_kwargs = dict(
            pretrained=False,
            amp = True),
        amp = True,
        use_vgg = True,
        dinov3_model_name = dinov3_model_name,
        amp_dtype=amp_dtype,
    )
    h,w = resolution
    symmetric = True
    attenuate_cert = True
    sample_mode = "threshold_balanced"
    matcher = RegressionMatcher(encoder, decoder, h=h, w=w, upsample_preds=upsample_preds, 
                                symmetric = symmetric, attenuate_cert = attenuate_cert, sample_mode = sample_mode, **kwargs).to(device)
    
    # ÏõêÎ≥∏ RoMa Í∞ÄÏ§ëÏπò Î°úÎìú (DINOv3 coarse level Ï†úÏô∏)
    if original_roma_weights is not None:
        matcher, skipped_keys = load_roma_weights_except_coarse(matcher, original_roma_weights)
        
        # proj16.0.weightÍ∞Ä Ïä§ÌÇµÎêòÏóàÏùÑ ÎïåÎßå Kaiming Ï¥àÍ∏∞Ìôî Ï†ÅÏö©
        if 'decoder.proj.16.0.weight' in skipped_keys:
            print("üîÑ proj16.0.weightÍ∞Ä Ïä§ÌÇµÎêòÏñ¥ Kaiming Ï¥àÍ∏∞Ìôî Ï†ÅÏö©")
            nn.init.kaiming_normal_(matcher.decoder.proj['16'][0].weight, mode='fan_out', nonlinearity='relu')
            if matcher.decoder.proj['16'][0].bias is not None:
                nn.init.constant_(matcher.decoder.proj['16'][0].bias, 0)
    else:
        # Í∞ÄÏ§ëÏπò Î°úÎìúÍ∞Ä ÏóÜÏùÑ ÎïåÎäî Ìï≠ÏÉÅ Kaiming Ï¥àÍ∏∞Ìôî Ï†ÅÏö©
        nn.init.kaiming_normal_(matcher.decoder.proj['16'][0].weight, mode='fan_out', nonlinearity='relu')
        if matcher.decoder.proj['16'][0].bias is not None:
            nn.init.constant_(matcher.decoder.proj['16'][0].bias, 0)
    
    # Ï∂îÍ∞Ä Í∞ÄÏ§ëÏπò Î°úÎìú (ÏûàÎäî Í≤ΩÏö∞)
    if weights is not None:
        matcher.load_state_dict(weights)
    
    return matcher