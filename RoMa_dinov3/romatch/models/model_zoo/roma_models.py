import warnings
import torch.nn as nn
import torch
from romatch.models.matcher import *
from romatch.models.transformer import Block, TransformerDecoder, MemEffAttention
from romatch.models.encoders import *
from romatch.models.encoders import CNNandDinov3
from romatch.models.tiny import TinyRoMa

def tiny_roma_v1_model(weights = None, freeze_xfeat=False, exact_softmax=False, xfeat = None):
    model = TinyRoMa(
        xfeat = xfeat,
        freeze_xfeat=freeze_xfeat, 
        exact_softmax=exact_softmax)
    if weights is not None:
        model.load_state_dict(weights)
    return model

def load_roma_weights_except_coarse(model, original_weights_path):
    """
    ì›ë³¸ RoMa ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ë˜, coarse level (DINOv3 ë¶€ë¶„)ì€ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ë¶€ë¶„ë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print("ğŸ”„ ì›ë³¸ RoMa ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘ (DINOv3 coarse level ì œì™¸)...")
    
    # ì›ë³¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    original_weights = torch.load(original_weights_path, map_location='cpu')
    
    # í˜„ì¬ ëª¨ë¸ì˜ state_dict ê°€ì ¸ì˜¤ê¸°
    current_state_dict = model.state_dict()
    
    # ë¡œë“œí•  ê°€ì¤‘ì¹˜ í•„í„°ë§ (DINOv3 ê´€ë ¨ ë¶€ë¶„ ì œì™¸)
    filtered_weights = {}
    skipped_keys = []
    
    for key, value in original_weights.items():
        # DINOv3 ê´€ë ¨ í‚¤ë“¤ ì œì™¸ (ì²´í¬í¬ì¸íŠ¸ selective ë¡œë”©)
        if any(skip_pattern in key for skip_pattern in [
            'encoder.cnn.',  # VGG19 ë¶€ë¶„ì€ ì œì™¸ (DINOv3ë¡œ ëŒ€ì²´ë¨)
            'encoder.dinov2_model',  # DINOv2 ëª¨ë¸ ë¶€ë¶„ (ê¸°ì¡´)
            'encoder.dinov3_model',  # DINOv3 ëª¨ë¸ ë¶€ë¶„ (ìƒˆë¡œ ì¶”ê°€)
            'encoder._dinov3_model',  # DINOv3 ëª¨ë¸ ë¶€ë¶„ (ìƒˆë¡œ ì¶”ê°€)
            # 'decoder.proj.16.0.weight',  # proj16ì€ ë™ì  ì„¤ì •ìœ¼ë¡œ ë¡œë“œ ê°€ëŠ¥
        ]):
            skipped_keys.append(key)
            continue
            
        # í˜„ì¬ ëª¨ë¸ì— í•´ë‹¹ í‚¤ê°€ ìˆê³  í¬ê¸°ê°€ ë§ëŠ” ê²½ìš°ë§Œ ë¡œë“œ
        if key in current_state_dict:
            if current_state_dict[key].shape == value.shape:
                filtered_weights[key] = value
            else:
                # proj16 í‚¤ëŠ” ê°•ì œ ë¡œë“œ/ë³€í™˜ (ë™ì  ì°¨ì›ìœ¼ë¡œ ì¸í•œ í¬ê¸° ë¶ˆì¼ì¹˜ í—ˆìš©)
                if 'proj.16' in key and value.shape[1] != current_state_dict[key].shape[1]:
                    print(f"  âš ï¸  proj16 í¬ê¸° ë¶ˆì¼ì¹˜ë¡œ ê±´ë„ˆëœ€: {key} (ì›ë³¸: {value.shape}, í˜„ì¬: {current_state_dict[key].shape})")
                    skipped_keys.append(key)
                    continue
                else:
                    print(f"  âš ï¸  í¬ê¸° ë¶ˆì¼ì¹˜ë¡œ ê±´ë„ˆëœ€: {key} (ì›ë³¸: {value.shape}, í˜„ì¬: {current_state_dict[key].shape})")
                    skipped_keys.append(key)
        else:
            print(f"  âš ï¸  í‚¤ê°€ ì—†ì–´ì„œ ê±´ë„ˆëœ€: {key}")
            skipped_keys.append(key)
    
    # í•„í„°ë§ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(filtered_weights, strict=False)
    
    print(f"âœ… ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ!")
    print(f"  - ë¡œë“œëœ í‚¤: {len(filtered_weights)}ê°œ")
    print(f"  - ê±´ë„ˆë›´ í‚¤: {len(skipped_keys)}ê°œ")
    
    if skipped_keys:
        print(f"  - ê±´ë„ˆë›´ í‚¤ë“¤: {skipped_keys[:5]}{'...' if len(skipped_keys) > 5 else ''}")
    
    return model, skipped_keys

def roma_model(resolution, upsample_preds, device = None, weights=None, dinov3_model_name="facebook/dinov3-vit7b16-pretrain-sat493m", amp_dtype: torch.dtype=torch.float16, original_roma_weights=None, **kwargs):
    # ì…ë ¥ í•´ìƒë„ 512x512ë¡œ ë³€ê²½ (íŒ¨ì¹˜ 16 ë°°ìˆ˜)
    resolution = (512, 512)
    h, w = resolution
    
    # romatch weights and dinov3 weights are loaded seperately, as dinov3 weights are not parameters
    #torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul TODO: these probably ruin stuff, should be careful
    #torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn
    warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')
    gp_dim = 512
    feat_dim = 512
    decoder_dim = gp_dim + feat_dim
    cls_to_coord_res = 64
    coordinate_decoder = TransformerDecoder(
        nn.Sequential(*[Block(decoder_dim, 8, attn_class=MemEffAttention) for _ in range(5)]), 
        decoder_dim, 
        cls_to_coord_res**2 + 1,
        is_classifier=True,
        amp = True,
        pos_enc = False,)
    dw = True
    hidden_blocks = 8
    kernel_size = 5
    displacement_emb = "linear"
    disable_local_corr_grad = True
    
    conv_refiner = nn.ModuleDict(
        {
            "16": ConvRefiner(
                2 * 512+128+(2*7+1)**2,
                2 * 512+128+(2*7+1)**2,
                2 + 1,
                kernel_size=kernel_size,
                dw=dw,
                hidden_blocks=hidden_blocks,
                displacement_emb=displacement_emb,
                displacement_emb_dim=128,
                local_corr_radius = 7,
                corr_in_other = True,
                amp = True,
                disable_local_corr_grad = disable_local_corr_grad,
                bn_momentum = 0.01,
            ),
            "8": ConvRefiner(
                2 * 512+64+(2*3+1)**2,
                2 * 512+64+(2*3+1)**2,
                2 + 1,
                kernel_size=kernel_size,
                dw=dw,
                hidden_blocks=hidden_blocks,
                displacement_emb=displacement_emb,
                displacement_emb_dim=64,
                local_corr_radius = 3,
                corr_in_other = True,
                amp = True,
                disable_local_corr_grad = disable_local_corr_grad,
                bn_momentum = 0.01,
            ),
            "4": ConvRefiner(
                2 * 256+32+(2*2+1)**2,
                2 * 256+32+(2*2+1)**2,
                2 + 1,
                kernel_size=kernel_size,
                dw=dw,
                hidden_blocks=hidden_blocks,
                displacement_emb=displacement_emb,
                displacement_emb_dim=32,
                local_corr_radius = 2,
                corr_in_other = True,
                amp = True,
                disable_local_corr_grad = disable_local_corr_grad,
                bn_momentum = 0.01,
            ),
            "2": ConvRefiner(
                2 * 64+16,
                128+16,
                2 + 1,
                kernel_size=kernel_size,
                dw=dw,
                hidden_blocks=hidden_blocks,
                displacement_emb=displacement_emb,
                displacement_emb_dim=16,
                amp = True,
                disable_local_corr_grad = disable_local_corr_grad,
                bn_momentum = 0.01,
            ),
            "1": ConvRefiner(
                2 * 9 + 6,
                24,
                2 + 1,
                kernel_size=kernel_size,
                dw=dw,
                hidden_blocks = hidden_blocks,
                displacement_emb = displacement_emb,
                displacement_emb_dim = 6,
                amp = True,
                disable_local_corr_grad = disable_local_corr_grad,
                bn_momentum = 0.01,
            ),
        }
    )
    kernel_temperature = 0.2
    learn_temperature = False
    no_cov = True
    kernel = CosKernel
    only_attention = False
    basis = "fourier"
    gp16 = GP(
        kernel,
        T=kernel_temperature,
        learn_temperature=learn_temperature,
        only_attention=only_attention,
        gp_dim=gp_dim,
        basis=basis,
        no_cov=no_cov,
    )
    gps = nn.ModuleDict({"16": gp16})
    
    # ë™ì  proj16 (DINOv3 ì°¨ì› 4096 ì ì‘)
    from transformers import AutoModel
    dinov3_model = AutoModel.from_pretrained(dinov3_model_name)
    hidden_size = dinov3_model.config.hidden_size  # 4096
    proj16 = nn.Sequential(nn.Conv2d(hidden_size, 512, 1, 1), nn.BatchNorm2d(512))
    proj8 = nn.Sequential(nn.Conv2d(512, 512, 1, 1), nn.BatchNorm2d(512))
    proj4 = nn.Sequential(nn.Conv2d(256, 256, 1, 1), nn.BatchNorm2d(256))
    proj2 = nn.Sequential(nn.Conv2d(128, 64, 1, 1), nn.BatchNorm2d(64))
    proj1 = nn.Sequential(nn.Conv2d(64, 9, 1, 1), nn.BatchNorm2d(9))
    proj = nn.ModuleDict({
        "16": proj16,
        "8": proj8,
        "4": proj4,
        "2": proj2,
        "1": proj1,
        })
    displacement_dropout_p = 0.0
    gm_warp_dropout_p = 0.0
    decoder = Decoder(coordinate_decoder, 
                      gps, 
                      proj, 
                      conv_refiner, 
                      detach=True, 
                      scales=["16", "8", "4", "2", "1"], 
                      displacement_dropout_p = displacement_dropout_p,
                      gm_warp_dropout_p = gm_warp_dropout_p)
    
    encoder = CNNandDinov3(
        cnn_kwargs = dict(
            pretrained=False,
            amp = True),
        amp = True,
        use_vgg = True,
        dinov3_model_name = dinov3_model_name,
        amp_dtype=amp_dtype,
    )
    h,w = resolution
    symmetric = True
    attenuate_cert = True
    sample_mode = "threshold_balanced"
    matcher = RegressionMatcher(encoder, decoder, h=h, w=w, upsample_preds=upsample_preds, 
                                symmetric = symmetric, attenuate_cert = attenuate_cert, sample_mode = sample_mode, **kwargs).to(device)
    
    # ì›ë³¸ RoMa ê°€ì¤‘ì¹˜ ë¡œë“œ (DINOv3 coarse level ì œì™¸)
    if original_roma_weights is not None:
        matcher, skipped_keys = load_roma_weights_except_coarse(matcher, original_roma_weights)
        
        # proj16.0.weightê°€ ìŠ¤í‚µë˜ì—ˆì„ ë•Œë§Œ Kaiming ì´ˆê¸°í™” ì ìš©
        if 'decoder.proj.16.0.weight' in skipped_keys:
            print("ğŸ”„ proj16.0.weightê°€ ìŠ¤í‚µë˜ì–´ Kaiming ì´ˆê¸°í™” ì ìš©")
            nn.init.kaiming_normal_(matcher.decoder.proj['16'][0].weight, mode='fan_out', nonlinearity='relu')
            if matcher.decoder.proj['16'][0].bias is not None:
                nn.init.constant_(matcher.decoder.proj['16'][0].bias, 0)
    else:
        # ê°€ì¤‘ì¹˜ ë¡œë“œê°€ ì—†ì„ ë•ŒëŠ” í•­ìƒ Kaiming ì´ˆê¸°í™” ì ìš©
        nn.init.kaiming_normal_(matcher.decoder.proj['16'][0].weight, mode='fan_out', nonlinearity='relu')
        if matcher.decoder.proj['16'][0].bias is not None:
            nn.init.constant_(matcher.decoder.proj['16'][0].bias, 0)
    
    # ì¶”ê°€ ê°€ì¤‘ì¹˜ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    if weights is not None:
        matcher.load_state_dict(weights)
    
    return matcher